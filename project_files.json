{
    "fraud_train.py": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\nfrom imblearn.over_sampling import SMOTE\nimport os\nimport joblib\n\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    XGBOOST_AVAILABLE = False\n    print(\"Warning: XGBoost not installed or failed to import. Skipping XGBoost model.\")\nexcept Exception as e:\n    XGBOOST_AVAILABLE = False\n    print(f\"Warning: XGBoost failed to import: {e}. Skipping XGBoost model.\")\n\n# 1. Data Loading\ndef load_data(filepath='cleaned_creditcard.csv'):\n    if os.path.exists(filepath):\n        print(f\"Loading dataset from {filepath}...\")\n        df = pd.read_csv(filepath)\n    else:\n        print(\"Dataset not found. Generating synthetic data for demonstration...\")\n        # Generate synthetic data mimicking the structure\n        n_samples = 10000\n        n_features = 30 # V1-V28 + Time + Amount\n        \n        # Create random features\n        X = np.random.randn(n_samples, n_features)\n        \n        # Create imbalanced target\n        y = np.zeros(n_samples)\n        n_fraud = int(n_samples * 0.002) # 0.2% fraud\n        fraud_indices = np.random.choice(n_samples, n_fraud, replace=False)\n        y[fraud_indices] = 1\n        \n        columns = ['Time'] + [f'V{i}' for i in range(1, 29)] + ['Amount', 'Class']\n        \n        # Adjust Time and Amount to look somewhat realistic\n        X[:, 0] = np.cumsum(np.random.exponential(10, n_samples)) # Time\n        X[:, -1] = np.random.exponential(100, n_samples) # Amount\n        \n        data = np.column_stack((X, y))\n        df = pd.DataFrame(data, columns=columns)\n        \n    print(f\"Data shape: {df.shape}\")\n    print(f\"Class distribution:\\n{df['Class'].value_counts(normalize=True)}\")\n    return df\n\n# 2. Preprocessing\ndef preprocess_data(df):\n    print(\"\\nPreprocessing data...\")\n    \n    # Handle missing values (simple imputation if any)\n    if df.isnull().sum().sum() > 0:\n        print(\"Handling missing values...\")\n        df = df.fillna(df.mean())\n\n    # Normalize Time and Amount\n    # RobustScaler is less prone to outliers\n    rob_scaler_amount = RobustScaler()\n    rob_scaler_time = RobustScaler()\n    \n    df['scaled_amount'] = rob_scaler_amount.fit_transform(df['Amount'].values.reshape(-1,1))\n    df['scaled_time'] = rob_scaler_time.fit_transform(df['Time'].values.reshape(-1,1))\n    \n    df.drop(['Time', 'Amount'], axis=1, inplace=True)\n    \n    # Move scaled columns to front for easier slicing if needed, or just keep as is\n    # Let's just use all columns except Class as features\n    \n    X = df.drop('Class', axis=1)\n    y = df['Class']\n    \n    return X, y, rob_scaler_amount, rob_scaler_time\n\n# 3. Splitting and Balancing\ndef split_and_balance(X, y):\n    print(\"\\nSplitting data...\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    print(\"Applying SMOTE to training set...\")\n    sm = SMOTE(random_state=42)\n    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n    \n    print(f\"Original train shape: {y_train.shape}, Fraud count: {sum(y_train)}\")\n    print(f\"Resampled train shape: {y_train_res.shape}, Fraud count: {sum(y_train_res)}\")\n    \n    return X_train_res, X_test, y_train_res, y_test\n\n# 4. Model Training and Evaluation\ndef train_evaluate_models(X_train, X_test, y_train, y_test):\n    models = {\n        'Logistic Regression': LogisticRegression(max_iter=1000),\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n    }\n    \n    if XGBOOST_AVAILABLE:\n        models['XGBoost'] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n    \n    results = {}\n    \n    plt.figure(figsize=(10, 8))\n    \n    for name, model in models.items():\n        print(f\"\\nTraining {name}...\")\n        model.fit(X_train, y_train)\n        \n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n        \n        # Metrics\n        print(f\"--- {name} Evaluation ---\")\n        print(classification_report(y_test, y_pred))\n        auc = roc_auc_score(y_test, y_pred_proba)\n        print(f\"ROC AUC: {auc:.4f}\")\n        \n        results[name] = {\n            'model': model,\n            'auc': auc,\n            'y_pred': y_pred,\n            'y_pred_proba': y_pred_proba\n        }\n        \n        # ROC Curve\n        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})')\n        \n        # Confusion Matrix\n        cm = confusion_matrix(y_test, y_pred)\n        print(f\"Confusion Matrix:\\n{cm}\")\n\n        # Save Random Forest model\n        if name == 'Random Forest':\n            joblib.dump(model, 'random_forest_model.pkl')\n            print(f\"Random Forest model saved to random_forest_model.pkl\")\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves')\n    plt.legend()\n    plt.savefig('roc_curves.png')\n    print(\"\\nROC Curves saved to roc_curves.png\")\n    \n    return results\n\n# 5. Feature Importance\ndef plot_feature_importance(results, feature_names):\n    print(\"\\nPlotting feature importances...\")\n    \n    for name, result in results.items():\n        model = result['model']\n        if hasattr(model, 'feature_importances_'):\n            importances = model.feature_importances_\n            indices = np.argsort(importances)[::-1]\n            \n            plt.figure(figsize=(12, 6))\n            plt.title(f\"Feature Importances - {name}\")\n            plt.bar(range(len(indices)), importances[indices], align=\"center\")\n            # feature_names is a pandas Index, so we can index it directly\n            plt.xticks(range(len(indices)), feature_names[indices], rotation=90)\n            plt.tight_layout()\n            plt.savefig(f'feature_importance_{name.replace(\" \", \"_\")}.png')\n            print(f\"Saved feature_importance_{name.replace(' ', '_')}.png\")\n\ndef main():\n    df = load_data()\n    X, y, scaler_amount, scaler_time = preprocess_data(df)\n    \n    # Save scalers\n    joblib.dump(scaler_amount, 'scaler_amount.pkl')\n    joblib.dump(scaler_time, 'scaler_time.pkl')\n    print(\"Scalers saved to scaler_amount.pkl and scaler_time.pkl\")\n    \n    X_train, X_test, y_train, y_test = split_and_balance(X, y)\n    results = train_evaluate_models(X_train, X_test, y_train, y_test)\n    plot_feature_importance(results, X.columns)\n    print(\"\\nDone!\")\n\nif __name__ == \"__main__\":\n    main()\n",
    "app.py": "from flask import Flask, request, jsonify\nimport joblib\nimport pandas as pd\nimport numpy as np\n\napp = Flask(__name__)\n\n# Load model and scalers\ntry:\n    model = joblib.load('random_forest_model.pkl')\n    scaler_amount = joblib.load('scaler_amount.pkl')\n    scaler_time = joblib.load('scaler_time.pkl')\n    print(\"Model and scalers loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading model or scalers: {e}\")\n    model = None\n    scaler_amount = None\n    scaler_time = None\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if not model or not scaler_amount or not scaler_time:\n        return jsonify({'error': 'Model or scalers not loaded'}), 500\n\n    try:\n        data = request.get_json()\n        \n        # Create DataFrame from input\n        input_df = pd.DataFrame([data])\n        \n        # Check if all required columns are present\n        required_columns = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n        missing_cols = [col for col in required_columns if col not in input_df.columns]\n        if missing_cols:\n             return jsonify({'error': f'Missing columns: {missing_cols}'}), 400\n\n        # Preprocess\n        input_df['scaled_amount'] = scaler_amount.transform(input_df['Amount'].values.reshape(-1,1))\n        input_df['scaled_time'] = scaler_time.transform(input_df['Time'].values.reshape(-1,1))\n        \n        input_df.drop(['Time', 'Amount'], axis=1, inplace=True)\n        \n        # Ensure column order matches training (scaled_amount, scaled_time, V1...V28)\n        # In fraud_train.py, we dropped Time and Amount, and added scaled_amount and scaled_time.\n        # The order depends on where they were inserted or if they were appended.\n        # \"df['scaled_amount'] = ...\" appends to the end.\n        # So the order in X was: V1...V28, scaled_amount, scaled_time (since Time/Amount were dropped).\n        # Wait, let's check fraud_train.py logic again.\n        # df.drop(['Time', 'Amount'], axis=1, inplace=True)\n        # X = df.drop('Class', axis=1)\n        # So X columns are V1...V28, scaled_amount, scaled_time.\n        \n        # Let's reorder input_df to match\n        cols = [f'V{i}' for i in range(1, 29)] + ['scaled_amount', 'scaled_time']\n        input_df = input_df[cols]\n        \n        # Predict\n        prediction = model.predict(input_df)[0]\n        probability = model.predict_proba(input_df)[0][1]\n        \n        result = {\n            'prediction': int(prediction),\n            'probability': float(probability),\n            'status': 'Fraud' if prediction == 1 else 'Legitimate'\n        }\n        \n        return jsonify(result)\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(debug=True, port=5000)\n",
    "streamlit_app.py": "import streamlit as st\nimport joblib\nimport pandas as pd\nimport numpy as np\n\n# Set page configuration\nst.set_page_config(\n    page_title=\"Fraud Detection System\",\n    page_icon=\"\ud83d\udee1\ufe0f\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# Custom CSS for styling\nst.markdown(\"\"\"\n    <style>\n    .main {\n        background-color: #f5f5f5;\n    }\n    .stButton>button {\n        width: 100%;\n        background-color: #ff4b4b;\n        color: white;\n        font-weight: bold;\n    }\n    .stSuccess {\n        background-color: #d4edda;\n        color: #155724;\n        padding: 10px;\n        border-radius: 5px;\n    }\n    .stError {\n        background-color: #f8d7da;\n        color: #721c24;\n        padding: 10px;\n        border-radius: 5px;\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\n# Load model and scalers\n@st.cache_resource\ndef load_resources():\n    try:\n        model = joblib.load('random_forest_model.pkl')\n        scaler_amount = joblib.load('scaler_amount.pkl')\n        scaler_time = joblib.load('scaler_time.pkl')\n        return model, scaler_amount, scaler_time\n    except Exception as e:\n        st.error(f\"Error loading resources: {e}\")\n        return None, None, None\n\nmodel, scaler_amount, scaler_time = load_resources()\n\n# Sidebar\nst.sidebar.title(\"Navigation\")\nst.sidebar.info(\"This application uses a Random Forest model to detect fraudulent credit card transactions.\")\n\n# Main content\nst.title(\"\ud83d\udee1\ufe0f Credit Card Fraud Detection\")\n\ntab1, tab2 = st.tabs([\"Manual Entry\", \"Batch Upload (CSV)\"])\n\nwith tab1:\n    st.markdown(\"Enter transaction details below to check for potential fraud.\")\n\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.subheader(\"Transaction Details\")\n        time_val = st.number_input(\"Time (Seconds since first transaction)\", min_value=0.0, value=0.0)\n        amount_val = st.number_input(\"Amount ($)\", min_value=0.0, value=100.0)\n\n    with col2:\n        st.subheader(\"Anonymized Features (V1-V28)\")\n        st.caption(\"These features are PCA-transformed for privacy. You can generate random values for testing.\")\n        \n        if st.button(\"Generate Random Features\"):\n            random_features = np.random.randn(28)\n            st.session_state['features'] = random_features\n        \n        if 'features' not in st.session_state:\n            st.session_state['features'] = np.zeros(28)\n        \n        # Display features in an expander to save space\n        with st.expander(\"View/Edit Features V1-V28\"):\n            features = []\n            for i in range(28):\n                val = st.number_input(f\"V{i+1}\", value=float(st.session_state['features'][i]), key=f\"v{i+1}\")\n                features.append(val)\n\n    if st.button(\"Analyze Transaction\", type=\"primary\"):\n        if model and scaler_amount and scaler_time:\n            # Prepare input data\n            input_data = pd.DataFrame([features], columns=[f'V{i}' for i in range(1, 29)])\n            \n            # Scale Time and Amount\n            scaled_amount = scaler_amount.transform([[amount_val]])\n            scaled_time = scaler_time.transform([[time_val]])\n            \n            input_data['scaled_amount'] = scaled_amount.flatten()\n            input_data['scaled_time'] = scaled_time.flatten()\n            \n            # Reorder\n            cols = [f'V{i}' for i in range(1, 29)] + ['scaled_amount', 'scaled_time']\n            final_input = input_data[cols]\n            \n            # Predict\n            prediction = model.predict(final_input)[0]\n            probability = model.predict_proba(final_input)[0][1]\n            \n            st.markdown(\"---\")\n            st.subheader(\"Analysis Result\")\n            \n            if prediction == 1:\n                st.error(f\"\ud83d\udea8 FRAUD DETECTED! (Probability: {probability:.2%})\")\n                st.markdown(\"This transaction shows patterns consistent with fraudulent activity.\")\n            else:\n                st.success(f\"\u2705 Legitimate Transaction (Probability of Fraud: {probability:.2%})\")\n                st.markdown(\"This transaction appears to be safe.\")\n                \n        else:\n            st.error(\"Model or scalers not loaded. Please check the server logs.\")\n\nwith tab2:\n    st.header(\"Upload Transactions\")\n    st.markdown(\"Upload a CSV file containing transaction details. Required columns: `Time`, `Amount`, `V1`...`V28`.\")\n    \n    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n    \n    if uploaded_file is not None:\n        try:\n            df = pd.read_csv(uploaded_file)\n            st.write(\"Preview of uploaded data:\", df.head())\n            \n            # Validation\n            required_cols = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)]\n            missing_cols = [col for col in required_cols if col not in df.columns]\n            \n            if missing_cols:\n                st.error(f\"Missing columns: {missing_cols}\")\n            else:\n                if st.button(\"Analyze Batch\"):\n                    if model and scaler_amount and scaler_time:\n                        # Preprocess\n                        process_df = df.copy()\n                        \n                        # Scale\n                        process_df['scaled_amount'] = scaler_amount.transform(process_df['Amount'].values.reshape(-1,1))\n                        process_df['scaled_time'] = scaler_time.transform(process_df['Time'].values.reshape(-1,1))\n                        \n                        # Select and reorder for model\n                        cols = [f'V{i}' for i in range(1, 29)] + ['scaled_amount', 'scaled_time']\n                        X = process_df[cols]\n                        \n                        # Predict\n                        predictions = model.predict(X)\n                        probabilities = model.predict_proba(X)[:, 1]\n                        \n                        # Add results to original df\n                        df['Fraud_Prediction'] = predictions\n                        df['Fraud_Probability'] = probabilities\n                        df['Status'] = df['Fraud_Prediction'].apply(lambda x: 'Fraud' if x == 1 else 'Legitimate')\n                        \n                        st.success(\"Analysis Complete!\")\n                        st.dataframe(df)\n                        \n                        # Download button\n                        csv = df.to_csv(index=False).encode('utf-8')\n                        st.download_button(\n                            label=\"Download Results as CSV\",\n                            data=csv,\n                            file_name='fraud_predictions.csv',\n                            mime='text/csv',\n                        )\n                        \n                        # Summary metrics\n                        fraud_count = df['Fraud_Prediction'].sum()\n                        col_m1, col_m2 = st.columns(2)\n                        col_m1.metric(\"Total Transactions\", len(df))\n                        col_m2.metric(\"Fraudulent Transactions Detected\", int(fraud_count))\n                        \n                    else:\n                        st.error(\"Model resources not loaded.\")\n                        \n        except Exception as e:\n            st.error(f\"Error processing file: {e}\")\n",
    "requirements.txt": "pandas\nnumpy\nscikit-learn\nimbalanced-learn\nmatplotlib\nseaborn\nxgboost\nflask\nrequests\nstreamlit\n",
    "README.md": "# Credit Card Fraud Detection\n\n## Overview\nThis project implements a machine learning system to detect fraudulent credit card transactions. It uses a dataset of transactions, preprocesses the data (scaling, handling imbalance with SMOTE), and trains multiple models (Logistic Regression, Random Forest, XGBoost) to classify transactions as fraudulent or legitimate.\n\n## Files\n- **`fraud_train.py`**: The main script for training and evaluating models.\n- **`app.py`**: Flask API for serving the model.\n- **`streamlit_app.py`**: Streamlit web interface.\n- **`test_api.py`**: Script to test the API.\n- **`cleaned_creditcard.csv`**: The dataset used for training.\n- **`requirements.txt`**: List of Python dependencies.\n- **`random_forest_model.pkl`**: The saved Random Forest model.\n- **`scaler_amount.pkl`, `scaler_time.pkl`**: Saved scalers.\n- **`roc_curves.png`**: ROC curves comparing model performance.\n- **`feature_importance_*.png`**: Feature importance plots.\n\n## Setup & Usage\n\n### 1. Install Dependencies\nEnsure you have Python installed. Install the required packages using pip:\n\n```bash\npip install -r requirements.txt\n```\n\n*Note for macOS users: If you encounter issues with XGBoost, you may need to install `libomp`:*\n```bash\nbrew install libomp\n```\n\n### 2. Run the Training Script\nExecute the main script to train the models and generate results:\n\n```bash\npython3 fraud_train.py\n```\n\n### 3. Run the API\nStart the Flask server:\n\n```bash\npython3 app.py\n```\n\nSend a test request:\n```bash\npython3 test_api.py\n```\n\n### 4. Run the Streamlit App\nStart the interactive UI:\n\n```bash\nstreamlit run streamlit_app.py\n```\n\n## Results\nThe system evaluates models using Precision, Recall, F1-score, and ROC-AUC.\n- **Random Forest** and **XGBoost** typically achieve high ROC-AUC scores (>0.96).\n- The trained Random Forest model is automatically saved as `random_forest_model.pkl`.\n- The API provides real-time predictions for new transactions.\n",
    "test_api.py": "import requests\nimport json\nimport numpy as np\n\n# URL of the Flask API\nurl = 'http://127.0.0.1:5000/predict'\n\n# Generate random sample data\n# Features: Time, Amount, V1-V28\ndata = {\n    'Time': 1000,\n    'Amount': 50.0\n}\nfor i in range(1, 29):\n    data[f'V{i}'] = np.random.randn()\n\n# Send POST request\ntry:\n    response = requests.post(url, json=data)\n    \n    print(f\"Status Code: {response.status_code}\")\n    print(\"Response JSON:\")\n    print(json.dumps(response.json(), indent=4))\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n",
    "fraud_detection.py": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, f1_score\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\ndef load_data(filepath):\n    \"\"\"Loads the dataset from the given filepath.\"\"\"\n    print(f\"Loading data from {filepath}...\")\n    try:\n        df = pd.read_csv(filepath)\n        print(f\"Data loaded successfully. Shape: {df.shape}\")\n        return df\n    except FileNotFoundError:\n        print(f\"Error: File not found at {filepath}\")\n        return None\n\ndef preprocess_data(df):\n    \"\"\"Preprocesses the data: scaling and splitting.\"\"\"\n    print(\"Preprocessing data...\")\n    \n    # RobustScaler is less prone to outliers\n    rob_scaler = RobustScaler()\n\n    df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n    df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\n    df.drop(['Time','Amount'], axis=1, inplace=True)\n    \n    # Move scaled columns to the front for easier access (optional but good for inspection)\n    scaled_amount = df['scaled_amount']\n    scaled_time = df['scaled_time']\n    \n    df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n    df.insert(0, 'scaled_amount', scaled_amount)\n    df.insert(1, 'scaled_time', scaled_time)\n\n    X = df.drop('Class', axis=1)\n    y = df['Class']\n\n    return X, y\n\ndef train_models(X_train, y_train):\n    \"\"\"Trains Logistic Regression, Random Forest, and XGBoost models.\"\"\"\n    print(\"Training models...\")\n    \n    models = {}\n    \n    # Logistic Regression\n    print(\"Training Logistic Regression...\")\n    lr = LogisticRegression(solver='liblinear')\n    lr.fit(X_train, y_train)\n    models['Logistic Regression'] = lr\n    \n    # Random Forest\n    print(\"Training Random Forest...\")\n    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n    rf.fit(X_train, y_train)\n    models['Random Forest'] = rf\n    \n    # XGBoost\n    print(\"Training XGBoost...\")\n    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n    xgb_model.fit(X_train, y_train)\n    models['XGBoost'] = xgb_model\n    \n    return models\n\ndef evaluate_models(models, X_test, y_test):\n    \"\"\"Evaluates models and plots ROC curves.\"\"\"\n    print(\"Evaluating models...\")\n    \n    plt.figure(figsize=(10, 8))\n    \n    for name, model in models.items():\n        print(f\"\\n--- {name} ---\")\n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n        \n        print(classification_report(y_test, y_pred))\n        print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n        \n        # Confusion Matrix\n        cm = confusion_matrix(y_test, y_pred)\n        print(f\"Confusion Matrix:\\n{cm}\")\n        \n        # ROC Curve\n        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_pred_proba):.2f})')\n        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curves')\n    plt.legend()\n    plt.savefig('roc_curves.png')\n    print(\"ROC Curves saved to roc_curves.png\")\n\ndef plot_feature_importance(models, feature_names):\n    \"\"\"Plots feature importance for tree-based models.\"\"\"\n    print(\"Plotting feature importance...\")\n    \n    for name, model in models.items():\n        if name in ['Random Forest', 'XGBoost']:\n            plt.figure(figsize=(12, 6))\n            importances = model.feature_importances_\n            indices = np.argsort(importances)[::-1]\n            \n            plt.title(f'Feature Importances - {name}')\n            plt.bar(range(len(indices)), importances[indices], align='center')\n            plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)\n            plt.tight_layout()\n            plt.savefig(f'feature_importance_{name.replace(\" \", \"_\")}.png')\n            print(f\"Feature importance for {name} saved.\")\n\ndef main():\n    # Filepath\n    filepath = 'cleaned_creditcard.csv' # Assuming the file is in the same directory\n    \n    # Load Data\n    df = load_data(filepath)\n    if df is None:\n        return\n\n    # Preprocess\n    X, y = preprocess_data(df)\n    \n    # Split Data\n    print(\"Splitting data...\")\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # SMOTE\n    print(\"Applying SMOTE...\")\n    sm = SMOTE(random_state=42)\n    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n    print(f\"Original dataset shape {y_train.value_counts()}\")\n    print(f\"Resampled dataset shape {y_train_res.value_counts()}\")\n    \n    # Train Models\n    models = train_models(X_train_res, y_train_res)\n    \n    # Evaluate\n    evaluate_models(models, X_test, y_test)\n    \n    # Feature Importance\n    plot_feature_importance(models, X.columns)\n    \n    print(\"Fraud detection pipeline completed.\")\n\nif __name__ == \"__main__\":\n    main()\n"
}